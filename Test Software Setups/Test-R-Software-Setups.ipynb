{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Introduction\"><span class=\"toc-item-num\">1 - </span>Introduction</a></div><div class=\"lev1\"><a href=\"#Instructions-for-Running-This-Test-Workbook\"><span class=\"toc-item-num\">2 - </span>Instructions for Running This Test Workbook</a></div><div class=\"lev1\"><a href=\"#Test-importing-of-R-modules\"><span class=\"toc-item-num\">3 - </span>Test importing of <code>R</code> modules</a></div><div class=\"lev1\"><a href=\"#Set-CONSTANTS\"><span class=\"toc-item-num\">4 - </span>Set <em>CONSTANTS</em></a></div><div class=\"lev1\"><a href=\"#Test-SparkContext-&amp;-SQLContext\"><span class=\"toc-item-num\">5 - </span>Test <code>SparkContext</code> &amp; <code>SQLContext</code></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This workbook tests the setups necessary to run our Relation Data tutorials in __`R`__ and __`SparkR`__ (the `R` API of `Apache Spark`.\n",
    "\n",
    "We will test your installation of the below software. Please follow [__instructions__ here](https://github.com/ChicagoBoothAnalytics/RelationalData) to install such software if you are running on your personal computer.\n",
    "\n",
    "- __`R`__ v3.2.2 or later;\n",
    "\n",
    "- __`lubridate`__ `R` package for manipulating date-times data;\n",
    "\n",
    "- __`stringr`__ `R` package for manipulating character string data;\n",
    "\n",
    "- __`data.table`__ `R` package;\n",
    "\n",
    "- __`RPostgreSQL`__ `R` package for working with `PostgreSQL` databases through `R`;\n",
    "\n",
    "- __`sqldf`__ `R` package for running `SQL` commands in `R`;\n",
    "\n",
    "- __`Apache Spark` v1.6.0 or later__, pre-built for Hadoop 2.6 or later;\n",
    "\n",
    "- __`ggplot2`__ `R` package for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instructions for Running This Test Workbook\n",
    "\n",
    "If you are running on your personal Mac & Windows computer, __edit the values of the `SPARK_HOME` and `WINDOWS_JAVA_HOME` constants__ below to suit the relevant setups on your computer.\n",
    "\n",
    "In the navigation bar, go to __Cell__ and press __Run All__. This will run all of the commands below.\n",
    "\n",
    "__WHAT SUCCESS LOOKS LIKE__: running of all workbook cells __without error messages__ _(note: warning messages are okay)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test importing of `R` modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘lubridate’\n",
      "\n",
      "The following objects are masked from ‘package:data.table’:\n",
      "\n",
      "    hour, mday, month, quarter, wday, week, yday, year\n",
      "\n",
      "Loading required package: DBI\n",
      "Loading required package: gsubfn\n",
      "Loading required package: proto\n",
      "Warning message:\n",
      "In doTryCatch(return(expr), name, parentenv, handler): unable to load shared object '/Library/Frameworks/R.framework/Resources/modules//R_X11.so':\n",
      "  dlopen(/Library/Frameworks/R.framework/Resources/modules//R_X11.so, 6): Library not loaded: /opt/X11/lib/libSM.6.dylib\n",
      "  Referenced from: /Library/Frameworks/R.framework/Resources/modules//R_X11.so\n",
      "  Reason: image not foundCould not load tcltk.  Will use slower R code instead.\n",
      "Loading required package: RSQLite\n",
      "sqldf will default to using PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "library(data.table)\n",
    "library(lubridate)\n",
    "library(RPostgreSQL)\n",
    "library(sqldf)\n",
    "library(stringr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import visualization packages\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set _CONSTANTS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# detect system platform\n",
    "SYS_NAME <- Sys.info()['sysname']\n",
    "\n",
    "# detect if running on Amazon Web Services (AWS) Elastic MapReduce (EMR), local Mac or local Windows\n",
    "AWS_EMR_MODE <- SYS_NAME == 'Linux'\n",
    "MAC_OSX_LOCAL_MODE <- SYS_NAME == 'Darwin'\n",
    "WINDOWS_LOCAL_MODE <- SYS_NAME == 'Windows'\n",
    "\n",
    "# set Apache Spark-related constants, depending on AWS_EMR_MODE\n",
    "if (AWS_EMR_MODE) {                                   # if running Spark on AWS Elastic MapReduce (EMR) YARN cluster\n",
    "    SPARK_MODE <- 'yarn-client'\n",
    "    SPARK_HOME <- '/usr/lib/spark'                    # default Spark installation folder on AWS EMR master node\n",
    "} else {                                              # if running Spark on single machine\n",
    "    SPARK_MODE <- 'local[*]'\n",
    "    if (MAC_OSX_LOCAL_MODE) {                         # if running on Mac OS X\n",
    "        SPARK_HOME <- '/Applications/spark-1.6.0'     # *** CHANGE TO SUIT YOUR PERSONAL COMPUTER ***\n",
    "    } else if (WINDOWS_LOCAL_MODE) {    # if running on Windows\n",
    "        SPARK_HOME <- 'C:/Applications/spark-1.6.0'   # *** CHANGE TO SUIT YOUR PERSONAL COMPUTER ***\n",
    "    }   \n",
    "}                                              \n",
    "\n",
    "WINDOWS_JAVA_HOME <- 'C:/Program Files/Java/jre1.8.0_74'   # *** CHANGE TO SUIT YOUR (WINDOWS) COMPUTER ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `SparkContext` & `SQLContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing any existing 'SPARK_CLASSPATH' environment variable: done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "The following object is masked from ‘package:RSQLite’:\n",
      "\n",
      "    summary\n",
      "\n",
      "The following object is masked from ‘package:RPostgreSQL’:\n",
      "\n",
      "    summary\n",
      "\n",
      "The following objects are masked from ‘package:lubridate’:\n",
      "\n",
      "    hour, intersect, minute, month, quarter, second, year\n",
      "\n",
      "The following objects are masked from ‘package:data.table’:\n",
      "\n",
      "    between, hour, last, like, month, quarter, tables, year\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, filter, lag, na.omit, predict, sd, var\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    colnames, colnames<-, intersect, rank, rbind, sample, subset,\n",
      "    summary, table, transform\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /Applications/spark-1.6.0/bin/spark-submit   sparkr-shell /var/folders/_r/pclm3n6x2b54y_1frhn2jh3c0000gn/T//RtmpEGyrJ9/backend_port2e3a1aaecbe7 \n",
      "SparkContext:"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.api.java.JavaSparkContext id 0 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQLContext / HiveContext:"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Java ref type org.apache.spark.sql.hive.HiveContext id 2 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (!exists('sc')) {\n",
    "    \n",
    "    # set / clean up environment variables for Spark\n",
    "    Sys.setenv(SPARK_HOME=SPARK_HOME)\n",
    "    \n",
    "    if (AWS_EMR_MODE) {\n",
    "        Sys.setenv(SPARKR_SUBMIT_ARGS='\"--packages\" \"com.databricks:spark-csv_2.10:1.3.0\" \"sparkr-shell\"')\n",
    "    } else if (WINDOWS_LOCAL_MODE) {\n",
    "        Sys.setenv(JAVA_HOME=WINDOWS_JAVA_HOME)\n",
    "    }\n",
    "    \n",
    "    system('rm -f derby.log')\n",
    "    system('rm -f -r metastore_db')\n",
    "    \n",
    "    cat(\"Removing any existing 'SPARK_CLASSPATH' environment variable: \")\n",
    "    spark_classpath <- Sys.getenv('SPARK_CLASSPATH')\n",
    "    if (spark_classpath == '') {\n",
    "        cat('done!\\n')\n",
    "    } else {\n",
    "        cat(spark_classpath, 'done!\\n')\n",
    "    }\n",
    "    \n",
    "    # add SparkR to library search path\n",
    "    .libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'), .libPaths()))\n",
    "    \n",
    "    # load SparkR and set up SparkContext & HiveContext\n",
    "    library(SparkR)\n",
    "    \n",
    "    sc <- sparkR.init(\n",
    "        master=SPARK_MODE,\n",
    "        appName='Test-R-Software_SetUps',\n",
    "        sparkHome=SPARK_HOME)\n",
    "\n",
    "    if (WINDOWS_LOCAL_MODE) {\n",
    "        # if running on Windows, use SQLContext because HiveContext does not work on Windows\n",
    "        sqlc <- sparkRSQL.init(sc)\n",
    "    } else {\n",
    "        # if running on Mac or AWS EMR YARN, use more advanced HiveContext\n",
    "        # ref: http://stackoverflow.com/questions/33666545/what-is-difference-between-apache-spark-sqlcontext-vs-hivecontext\n",
    "        sqlc <- sparkRHive.init(sc)\n",
    "    }\n",
    "}\n",
    "\n",
    "cat('SparkContext:')\n",
    "sc\n",
    "\n",
    "cat('\\nSQLContext / HiveContext:')\n",
    "sqlc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_section_display": "block",
   "toc_threshold": 4,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1330.21px",
   "right": "20px",
   "top": "121.997px",
   "width": "187px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
